生成时间: 2025-08-22 16:24:22
==================================================

原始文章信息:
标题: 究竟会花落谁家？DeepSeek最新大模型瞄准了下一代国产AI芯片
链接: https://www.jiqizhixin.com/articles/2025-08-22-2
发布时间: Fri, 22 Aug 2025 13:42:01 +0800
摘要: 
正文内容: 究竟会花落谁家？DeepSeek最新大模型瞄准了下一代国产AI芯片

0%
展开列表
球首款AI原生游戏引擎再进化：GTA6再不来，我们就AI一个
今天
Mirage 2
KDD 2025 Best Paper Runner-Up | EI-BERT：超紧凑语言模型压缩框架
今天
EI-BERT
即梦推出“智能多帧”功能 突破AI视频长镜头创作瓶颈
今天
从实验室到餐桌：Robert Langer团队杨昕博士用新材料破解全球「隐性饥饿」
今天
AI for Science
那些让你「活人微死」的工作日，终于有救了
今天
腾讯
Cursor为Blackwell从零构建MXFP8内核，MoE层提速3.5倍，端到端训练提速1.5倍
今天
Cursor
谷歌Gemini一次提示能耗≈看9秒电视，专家：别太信，有误导性
今天
Gemini
从繁杂技巧到极简方案：ROLL团队带来RL4LLM新实践
今天
淘天集团
究竟会花落谁家？DeepSeek最新大模型瞄准了下一代国产AI芯片
今天
国产芯片
ICCV 2025 | 打造通用工具智能体的基石：北大提出ToolVQA数据集，引领多模态多步推理VQA新范式
今天
ToolVQA
刚刚，好莱坞特效师展示AI生成的中文科幻大片，成本只有330元
08月21日
百度蒸汽机（MuseSteamer）2.0
摆脱遥控器，波士顿动力人形机器人，开始「长脑子」干活了
08月21日
波士顿动力
微软AI CEO警告：我们需要警惕「看似有意识的AI」
08月21日
Mustafa Suleyman
ICCV 2025 | ECD：高质量合成图表数据集，提升开源MLLM图表理解能力
08月21日
ECDBench
通义APP上线官方知识库，首批覆盖教育、法律、金融等五大领域
08月21日
通义APP
AI Scientist生成的论文被指「剽窃」，回应称「未引用相关研究」，AI自动化科研还靠谱吗？
08月21日
Sakana AI
应届生看过来！上海AI Lab校招通道已开，100+岗位，700+offer，让科研理想照进现实！
08月21日
校园招聘
击败Meta登榜首：推理增强的文档排序模型ReasonRank来了
08月21日
千寻位置护航无人机表演，开启品牌多城联动新篇章
08月21日
千寻位置
刚刚，字节开源Seed-OSS-36B模型，512k上下文
08月21日
Seed-OSS-36B-Base
机器之心

原创

2小时前

究竟会花落谁家？DeepSeek最新大模型瞄准了下一代国产AI芯片

软件+硬件的全链路国产 AI 体系来了？

这几天，不论国内国外，人们都在关注 DeepSeek 发布的 V3.1 新模型。

它采用了全新的混合推理架构，让模型能在一个统一框架内支持「思考」与「非思考」两种模式。V3.1 通过训练后优化，在工具使用与编程、搜索等智能体任务上表现均获得了较大提升。

Deepseek V3.1 的很多基准测试结果已经陆续在 SWE-bench 等榜单上出现。此外，新模型在 Aider 多语言编程基准测试中得分超越了 Anthropic 的 Claude 4 Opus，同时还有显著的成本优势。

与 DeepSeek 自己此前的模型相比，V3.1 的性能提升显著，它解决问题需要更多步骤，但经过了思维链压缩训练，在任务表现持平的情况下，token 消耗量可以减少 20-50%，因此有效成本与 GPT-5 mini 相当。

除了模型性能的提升之外，值得关注的是，DeepSeek 昨天在其微信公众号文章介绍 DeepSeek V3.1 的时候，特意回复指出，UE8M0 FP8 是针对即将发布的下一代国产芯片设计的机制。

这就引发了人们的很多猜想。

根据 Hugging Face 的介绍文档，DeepSeek V3.1 的模型参数量为 685B，其在训练过程中采用了 UE8M0 FP8 缩放浮点格式，以确保与微缩放浮点格式的兼容性。

其中，E 和 M 分别代表指数（Exponent）和尾数（Mantissa）的位数，U 表示无符号（Unsigned），可能针对激活值的非负特性优化。因此，UE8M0 可能是指新模型应用的特殊量化策略。

所谓 FP8，其全称为 8-bit floating point（8 位浮点数），是一种超低精度的数据表示格式，用于深度学习中的训练与推理。相较于 FP32（单精度）或 FP16（半精度）等传统浮点格式，FP8 的主要优势包括如下，因此可以在尽量保持数值稳定性和模型精度的前提下，进一步降低存储和计算开销：

显著节省显存，比如 FP32 占 4 字节，FP16 占 2 字节，而 FP8 仅占 1 字节。当推理规模达到百亿甚至千亿参数时，节省极为可观；

提升计算效率，FP8 可以在硬件上实现更高的并行度，比如 NVIDIA Hopper GPU 的 FP8 Tensor Core 吞吐量是 FP16 的两倍；

保持模型精度，FP8 通过缩放因子以及混合精度训练，在多数场景下能接近 FP16/FP32 的精度。

近年来，除了 NVIDIA 之外，Meta、英特尔、AMD 等也都开始研究 FP8 训练与推理，有成为业界「新黄金标准」的趋势，核心思路在于「两个格式配合使用」。此次，DeepSeek V3.1 此次采用 UE8M0 FP8，意味着其开始在 FP8 技术栈上做自主创新。

从传统浮点数的表示来看，UE8M0 没有符号位和尾数位，8bit 全部用在了指数位。

根据很多人的猜测，UE8M0 只能表示非负数，将覆盖非常大的正数范围或者零；8bit 全部用于指数，代表了极宽的范围，尤其适合处理梯度、激活值等跨数量级变化极大的数据；没有尾数，代表了数值精度极低（在某个指数范围内无法表达中间值），误差也可能非常大。

此外，根据前文提到的要兼容微缩放浮点格式，这种格式的思路是在小块数据中引入外部缩放因子来补偿精度。因此 UE8M0 也可能采用这种思路，从而在国产芯片中实现低比特宽度存储和快速计算。

而在国内，包括华为、寒武纪在内多家厂商的新一代 AI 芯片都可以支持 FP8 格式，这也让它们再次成为业界和资本圈关注的焦点。其中华为提出的 HiFloat8 （HiF8）方案通过「单一格式 + 锥形精度（tapered precision）」的思路，能够兼顾精度和范围，覆盖正向和反向传播。

最后，很多人可能依然好奇，DeepSeek V3.1 是使用国产芯片训练的吗？

毕竟 DeepSeek R2 越来越近了，前几天英国《金融时报》的报道刚刚「预热」过一波：

      上周四，FT 说 DeepSeek R2 延迟是因为其使用了国产芯片进行训练，DeepSeek 随即否认。

目前看来，在 DeepSeek V3.1 上使用国产芯片训练的概率还比较小，UME8 M0 应该是为国产推理芯片优化所使用的机制。

不过既然 DeepSeek 这回已经明确指出了，我们可以期待未来国产开源大模型，针对华为昇腾、寒武纪等 AI 芯片实现专门优化，并大规模应用。

参考链接：

https://www.reuters.com/world/china/chinese-ai-startup-deepseek-releases-upgraded-model-with-domestic-chip-support-2025-08-21/

https://www.ft.com/content/eb984646-6320-4bfe-a78d-a1da2274b092

究竟会花落谁家？DeepSeek最新大模型瞄准了下一代国产AI芯片

0%
展开列表
球首款AI原生游戏引擎再进化：GTA6再不来，我们就AI一个
今天
Mirage 2
KDD 2025 Best Paper Runner-Up | EI-BERT：超紧凑语言模型压缩框架
今天
EI-BERT
即梦推出“智能多帧”功能 突破AI视频长镜头创作瓶颈
今天
从实验室到餐桌：Robert Langer团队杨昕博士用新材料破解全球「隐性饥饿」
今天
AI for Science
那些让你「活人微死」的工作日，终于有救了
今天
腾讯
Cursor为Blackwell从零构建MXFP8内核，MoE层提速3.5倍，端到端训练提速1.5倍
今天
Cursor
谷歌Gemini一次提示能耗≈看9秒电视，专家：别太信，有误导性
今天
Gemini
从繁杂技巧到极简方案：ROLL团队带来RL4LLM新实践
今天
淘天集团
究竟会花落谁家？DeepSeek最新大模型瞄准了下一代国产AI芯片
今天
国产芯片
ICCV 2025 | 打造通用工具智能体的基石：北大提出ToolVQA数据集，引领多模态多步推理VQA新范式
今天
ToolVQA
刚刚，好莱坞特效师展示AI生成的中文科幻大片，成本只有330元
08月21日
百度蒸汽机（MuseSteamer）2.0
摆脱遥控器，波士顿动力人形机器人，开始「长脑子」干活了
08月21日
波士顿动力
微软AI CEO警告：我们需要警惕「看似有意识的AI」
08月21日
Mustafa Suleyman
ICCV 2025 | ECD：高质量合成图表数据集，提升开源MLLM图表理解能力
08月21日
ECDBench
通义APP上线官方知识库，首批覆盖教育、法律、金融等五大领域
08月21日
通义APP
AI Scientist生成的论文被指「剽窃」，回应称「未引用相关研究」，AI自动化科研还靠谱吗？
08月21日
Sakana AI
应届生看过来！上海AI Lab校招通道已开，100+岗位，700+offer，让科研理想照进现实！
08月21日
校园招聘
击败Meta登榜首：推理增强的文档排序模型ReasonRank来了
08月21日
千寻位置护航无人机表演，开启品牌多城联动新篇章
08月21日
千寻位置
刚刚，字节开源Seed-OSS-36B模型，512k上下文
08月21日
Seed-OSS-36B-Base
机器之心

原创

2小时前

究竟会花落谁家？DeepSeek最新大模型瞄准了下一代国产AI芯片

软件+硬件的全链路国产 AI 体系来了？

这几天，不论国内国外，人们都在关注 DeepSeek 发布的 V3.1 新模型。

它采用了全新的混合推理架构，让模型能在一个统一框架内支持「思考」与「非思考」两种模式。V3.1 通过训练后优化，在工具使用与编程、搜索等智能体任务上表现均获得了较大提升。

Deepseek V3.1 的很多基准测试结果已经陆续在 SWE-bench 等榜单上出现。此外，新模型在 Aider 多语言编程基准测试中得分超越了 Anthropic 的 Claude 4 Opus，同时还有显著的成本优势。

与 DeepSeek 自己此前的模型相比，V3.1 的性能提升显著，它解决问题需要更多步骤，但经过了思维链压缩训练，在任务表现持平的情况下，token 消耗量可以减少 20-50%，因此有效成本与 GPT-5 mini 相当。

除了模型性能的提升之外，值得关注的是，DeepSeek 昨天在其微信公众号文章介绍 DeepSeek V3.1 的时候，特意回复指出，UE8M0 FP8 是针对即将发布的下一代国产芯片设计的机制。

这就引发了人们的很多猜想。

根据 Hugging Face 的介绍文档，DeepSeek V3.1 的模型参数量为 685B，其在训练过程中采用了 UE8M0 FP8 缩放浮点格式，以确保与微缩放浮点格式的兼容性。

其中，E 和 M 分别代表指数（Exponent）和尾数（Mantissa）的位数，U 表示无符号（Unsigned），可能针对激活值的非负特性优化。因此，UE8M0 可能是指新模型应用的特殊量化策略。

所谓 FP8，其全称为 8-bit floating point（8 位浮点数），是
==================================================

截图信息: 已获取页面截图
==================================================

生成的小红书文案:
标题: 🔥国产AI新突破！DeepSeek V3.1来了，专为下一代国产芯片打造！
内容: 姐妹们！国产AI又双叒叕有大动作了！🚀 DeepSeek最新发布的V3.1大模型简直太强了！

✨ 这次新模型采用了全新的混合推理架构，可以同时支持「思考」和「非思考」两种模式，简直就是AI界的双面达人！

👑 最厉害的是，在Aider多语言编程测试中，它居然超越了Anthropic的Claude 4 Opus！而且成本还超级低，性价比拉满有没有！

💡 相比之前的版本，V3.1解决问题需要更多步骤，但经过思维链压缩训练，在效果不变的情况下，token消耗量减少20-50%！这意味着什么？就是更省钱更高效啦！

🇨🇳 重点来了！DeepSeek官方透露，UE8M0 FP8是专门为下一代国产AI芯片设计的机制！这意味着什么？软件+硬件的全链路国产AI体系要来了吗？想想就激动！

📱 简单科普下：FP8是一种超低精度数据格式，比传统格式占用更少内存，计算更快更省电，特别适合大型AI模型。以后我们的手机和电脑可能会因为这项技术变得更智能更省电哦！

姐妹们，你们觉得国产AI能否弯道超车？评论区聊聊吧！👇
标签: #DeepSeek, #国产AI, #人工智能, #科技前沿, #AI芯片, #科技资讯, #技术创新, #国产科技
